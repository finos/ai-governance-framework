---
sequence: 1
title: Information Leaked To Hosted Model
layout: risk
doc-status: Draft
type: RC
external_risks:
  - OWASP-LLM_2025_LLM02  # OWASP LLM: Sensitive Information Disclosure
  - OWASP-ML_2023_ML03    # OWASP ML Model Inversion Attack
  - OWASP-ML_2023_ML04    # OWASP ML Membership Inference Attack
  - NIST-600_2024_2-04    # NIST 600.1: Data Privacy
  - NIST-600_2024_2-09    # NIST 600.1: Information Security
ffiec_references:
  - ffiec_sec_ii-information-security-program-management
  - ffiec_sec_iii-security-operations
  - ffiec_ots_risk-management
eu-ai_references:
  - eu-ai_c3-s2-a10  # III.S2.A10 Data and Data Governance
  - eu-ai_c3-s2-a13  # III.S2.A13 Transparency and Provision of Information to Deployers
  - eu-ai_c5-s2-a53  # V.S2.A53 Obligations for Providers of General-Purpose AI Models
---

The transmission of sensitive data to third-party hosted Large Language Model (LLM) platforms for inference or fine-tuning introduces a significant risk of information leakage for financial institutions. This risk encompasses the potential exposure of confidential organizational data, including customer Personally Identifiable Information (PII), Non-Public Market Information (NPMI), proprietary trading algorithms, internal risk assessments, and strategic plans. Such leakage can occur due to inadequate data governance, insufficient control measures within the hosted environment, or inherent characteristics of LLM technology. This aligns with cybersecurity concerns such as those highlighted in OWASP LLM02: Sensitive Information Disclosure.

A core challenge arises from the nature of interactions with external LLMs, which can be conceptualized as a **two-way trust boundary**. Neither the data inputted into the LLM nor the output received can be fully trusted by default. Inputs containing sensitive financial information may be retained or processed insecurely by the provider, while outputs may inadvertently reveal previously processed sensitive data, even if the immediate input prompt appears benign.

Several mechanisms unique to or amplified by LLMs contribute to this risk:

* **Model Memorization and Overfitting:** LLMs, particularly very large ones, can inadvertently [memorize](https://arxiv.org/pdf/2310.18362) and retain portions of the data they have processed during training, fine-tuning, or even through user interactions. [Overfitting](https://aws.amazon.com/what-is/overfitting/) during the training process can exacerbate this. Consequently, sensitive information—such as specific customer details from a CRM excerpt, conditions from a confidential loan agreement, or elements of a proprietary financial strategy fed into the model—might be recalled and disclosed in subsequent, unrelated sessions, potentially even to different users (cross-user leakage). Research has demonstrated the [scalable extraction of training data from production language models](https://arxiv.org/abs/2311.17035), highlighting the practical reality of this threat.
* **Prompt-Based Attacks:** As detailed in the "Prompt Injection" risk (ri-10), adversarial actors can craft malicious prompts to manipulate the LLM into revealing sensitive information it has access to or has previously memorized. This method directly targets the model's ability to recall and synthesize information.
* **Inadequate Data Sanitization and Filtering:** The risk of inadvertent disclosure significantly increases if the LLM service provider or the financial institution itself fails to implement robust data sanitization, anonymization, or filtering mechanisms for both data inputted into the model and outputs generated by it.

The risk profile can be further influenced by the provider's data handling practices and the specific services utilized:

* **Data Retention, Logging, and Usage Policies:** The terms of use, data processing addendums, and actual data handling practices of the LLM provider are critical. Without clear contractual agreements and technical assurances regarding data encryption, retention limits, purpose limitation for data usage (e.g., preventing use for future model training without consent), and secure deletion, financial institutions lose visibility and control over their sensitive data. Hosted models may not always provide transparent mechanisms for how input data is processed or retained, increasing the risk of persistent exposure.
* **Fine-Tuning on Proprietary Data:** Fine-tuning an LLM on a financial institution's proprietary datasets (e.g., internal reports, customer communication logs, specialized financial documents) can embed sensitive details deeply within the model. If this fine-tuning is performed by the third-party provider, or if access controls to the fine-tuned model are not stringently managed, this sensitive embedded information may become accessible to unauthorized individuals or through model queries, potentially violating principles of least privilege.

It is important to conduct thorough due diligence, as the level of data protection can vary significantly between LLM offerings. Enterprise-grade commercial LLMs may offer features like private endpoints, options to disable data retention for model training, data encryption in transit and at rest, and stronger contractual safeguards regarding data ownership and usage. Conversely, free or consumer-grade services might have less stringent data protection measures and may explicitly state that user data will be used for future model improvements. Regardless of the provider's tier, ongoing vigilance and adherence to internal data governance policies are paramount.

The consequences of such information leakage for a financial institution can be severe:
* **Breach of Data Privacy Regulations:** Unauthorized disclosure of PII can lead to significant fines under regulations like GDPR, CCPA, and others, alongside mandated customer notifications.
* **Violation of Financial Regulations:** Leakage of confidential customer information or market-sensitive data can breach specific financial industry regulations concerning data security and confidentiality (e.g., GLBA in the US).
* **Loss of Competitive Advantage:** Exposure of proprietary algorithms, trading strategies, or confidential business plans can erode a firm's competitive edge.
* **Reputational Damage:** Public disclosure of sensitive data leakage incidents can lead to a substantial loss of customer trust and damage to the institution's brand.
* **Legal Liabilities:** Beyond regulatory fines, institutions may face lawsuits from affected customers or partners.

Mitigating this risk requires a multi-layered approach, including rigorous provider due diligence, contractual safeguards, data minimization strategies, robust input/output validation and sanitization, exploring privacy-enhancing technologies, and continuous monitoring of data flows and model interactions.

#### Links

* [FFIEC IT Handbook](https://ithandbook.ffiec.gov/)
* [FFIEC Outsourcing Technology Services Appendix J](https://www.ffiec.gov/%5C/press/PDF/FFIEC_Appendix_J.pdf)
* [Scalable Extraction of Training Data from (Production) Language Models](https://arxiv.org/abs/2311.17035)
* [Who's Harry Potter? Approximate Unlearning in LLMs (Memorization Example)](https://arxiv.org/pdf/2310.18362)
* [OWASP LLM02: Sensitive Information Disclosure](https://genai.owasp.org/llmrisk/llm02-sensitive-information-disclosure/) (Note: The original text referred to LLM06, this link reflects the current OWASP designation for this risk)
* [Overfitting (AWS Explanation)](https://aws.amazon.com/what-is/overfitting/)
* [LLM Prompt Hacking (OWASP)](https://owasp.org/www-project-llm-prompt-hacking/)
