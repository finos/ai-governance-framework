---
sequence: 20
title: Reputational Risk
layout: risk
doc-status: Pre-Draft
type: OP
external_risks:
  - OWASP-LLM_2025_LLM09  # OWASP LLM: Misinformation
ffiec_references:
  - ffiec_mgt_ii-risk-management
  - ffiec_bcm_iii-risk-management
  - ffiec_aud_risk-assessment-and-risk-based-auditing
eu-ai_references:
  - eu-ai_c2-a5  # II.A5 Prohibited AI Practices
  - eu-ai_c3-s2-a9  # III.S2.A9 Risk Management System
  - eu-ai_c3-s2-a14  # III.S2.A14 Human Oversight
---

Failures, misuse, or unethical applications of Artificial Intelligence (AI) can rapidly escalate into public incidents, significantly eroding stakeholder trust and damaging an institution's reputation. This is particularly acute in the financial sector, where trust is a foundational pillar.

Customer-facing GenAI applications, such as chatbots or virtual assistants, carry a direct reputational risk if they generate offensive, inaccurate, biased, or otherwise inappropriate content. Negative press coverage and public backlash stemming from AI-driven unfair treatment, significant errors in financial advice, or data breaches can have severe and lasting consequences on a firm's brand and customer loyalty. High-profile errors, for instance, such as systematically biased loan application denials or the dissemination of flawed investor reports generated by AI, can be exceptionally damaging.

Compliance failures linked to the deployment or operation of AI systems can also lead to substantial regulatory fines, increased scrutiny, and further reputational harm. Regulators have increasingly highlighted AI-related reputational risk as a key concern for the financial services industry. Financial institutions must recognize that the outputs and actions of their AI-driven services are a direct reflection of their overall conduct and commitment to responsible practices.

A critical aspect of AI-related reputational risk is the potential for rapid scalability of errors. A flaw in an AI system could lead to the dissemination of incorrect or harmful messages to thousands, or even millions, of customers almost instantaneously. Therefore, damage to reputation arising from AI missteps constitutes a significant operational risk that requires proactive governance, rigorous testing, and continuous monitoring.
