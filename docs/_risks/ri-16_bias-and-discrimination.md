---
sequence: 16
title: Bias and Discrimination
layout: risk
doc-status: Approved-Specification
type: OP
nist-ai-600-1_references:
  - 2-6  # 2.6. Harmful Bias and Homogenization
ffiec-itbooklets_references:
  - mgt-2  # MGT: II Risk Management
  - dam-3  # DAM: III Risk Management of Development, Acquisition, and Maintenance
  - aud-4  # AUD: Risk Assessment and Risk-Based Auditing
eu-ai-act_references:
  - c2-a5      # II.A5 Prohibited AI Practices
  - c3-s2-a9   # III.S2.A9: Risk Management System
  - c3-s2-a14  # III.S2.A14: Human Oversight
related_risks:
  - ri-19  # Data Quality and Drift
  - ri-22  # Regulatory Compliance and Oversight
---

## Summary

AI systems can systematically disadvantage protected groups through biased training data, flawed design, or proxy variables that correlate with sensitive characteristics. In financial services, this manifests as discriminatory credit decisions, unfair fraud detection, or biased customer service, potentially violating fair lending laws and causing significant regulatory and reputational damage.

## Description

Within the financial services industry, the manifestations and consequences of AI-driven bias and discrimination can be particularly severe, impacting critical functions and leading to significant harm:

* **Biased Credit Scoring**:
  An AI model trained on historical lending data may learn patterns that reflect past discriminatory practices—such as granting loans disproportionately to individuals from certain zip codes, employment types, or educational backgrounds. This can result in lower credit scores for minority applicants or applicants from underserved communities, even if their actual financial behaviour is comparable to others.

* **Unfair Loan Approval Recommendations**:
  An LLM-powered decision support tool might assist underwriters by summarizing borrower applications. If trained on biased documentation or internal guidance, the system might consistently recommend rejection for certain profiles (e.g., single parents, freelancers), reinforcing systemic exclusion and contributing to disparate impact under fair lending laws.

* **Discriminatory Insurance Premium Calculations**:
  Insurance pricing algorithms that use AI may rely on features like occupation, home location, or education level—attributes that correlate with socioeconomic status or race. This can lead to higher premiums for certain demographic groups without a justifiable basis in actual risk, potentially violating fairness or equal treatment regulations.

* **Disparate Marketing Practices**:
  AI systems used for personalized financial product recommendations or targeted advertising might exclude certain users from seeing offers—such as mortgage refinancing or investment services—based on income, browsing behaviour, or inferred demographics. This results in unequal access to financial opportunities and can perpetuate wealth gaps.

* **Customer Service Disparities**:
  Foundational models used in customer support chatbots may respond differently based on linguistic patterns or perceived socioeconomic cues. For example, customers writing in non-standard English or with certain accents (in voice-based systems) might receive lower-quality or less helpful responses, affecting service equity.


### Root Causes of Bias

The root causes of bias in AI systems are multifaceted. They include:
* **Data Bias:** Training datasets may reflect historical societal biases or underrepresent certain populations, leading the model to learn and perpetuate these biases. For example, if a model is trained on historical loan data that shows a lower approval rate for a certain demographic, it may learn to replicate this bias, even if the underlying data is flawed.
* **Algorithmic Bias:** The choice of model architecture, features, and optimization functions can unintentionally introduce or amplify biases. For instance, an algorithm might inadvertently place more weight on a particular feature that is highly correlated with a protected characteristic, leading to biased outcomes.
* **Proxy Discrimination:** Seemingly neutral data points (e.g., postal codes, certain types of transaction history) can act as proxies for protected characteristics like race or socioeconomic status. A model might learn to associate these proxies with negative outcomes, leading to discriminatory decisions.
* **Feedback Loops:** If a biased AI system's outputs are fed back into its learning cycle without correction, the bias can become self-reinforcing and amplified over time. For example, if a biased fraud detection model flags certain transactions as fraudulent, and these flagged transactions are used to retrain the model, the model may become even more biased against those types of transactions in the future.

### Implications

The implications of deploying biased AI systems are far-reaching for financial institutions, encompassing:
* **Regulatory Sanctions and Legal Liabilities:** Severe penalties, fines, and legal action for non-compliance with anti-discrimination laws and financial regulations.
* **Reputational Damage:** Significant erosion of public trust, customer loyalty, and brand value.
* **Customer Detriment:** Direct harm to customers through unfair treatment, financial exclusion, or economic loss.
* **Operational Inefficiencies:** Flawed decision-making stemming from biased models can lead to suboptimal business outcomes and increased operational risk.

## Links

* [Wikipedia: Disparate impact](https://en.wikipedia.org/wiki/Disparate_impact)

